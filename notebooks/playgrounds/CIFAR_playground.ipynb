{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b1d58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/workspace/code/compositional-representation-learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c419f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from pl_modules.BoxEmbeddings.PatchBoxEmbeddingsVAE import VanillaVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7400b896",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = \"/home/ubuntu/workspace/data_root_dir\"\n",
    "\n",
    "ds = torchvision.datasets.CIFAR10(\n",
    "    root=data_root_dir, train=False, download=True, transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc26f704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ade28243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2314, 0.1686, 0.1961,  ..., 0.6196, 0.5961, 0.5804],\n",
       "         [0.0627, 0.0000, 0.0706,  ..., 0.4824, 0.4667, 0.4784],\n",
       "         [0.0980, 0.0627, 0.1922,  ..., 0.4627, 0.4706, 0.4275],\n",
       "         ...,\n",
       "         [0.8157, 0.7882, 0.7765,  ..., 0.6275, 0.2196, 0.2078],\n",
       "         [0.7059, 0.6784, 0.7294,  ..., 0.7216, 0.3804, 0.3255],\n",
       "         [0.6941, 0.6588, 0.7020,  ..., 0.8471, 0.5922, 0.4824]],\n",
       "\n",
       "        [[0.2431, 0.1804, 0.1882,  ..., 0.5176, 0.4902, 0.4863],\n",
       "         [0.0784, 0.0000, 0.0314,  ..., 0.3451, 0.3255, 0.3412],\n",
       "         [0.0941, 0.0275, 0.1059,  ..., 0.3294, 0.3294, 0.2863],\n",
       "         ...,\n",
       "         [0.6667, 0.6000, 0.6314,  ..., 0.5216, 0.1216, 0.1333],\n",
       "         [0.5451, 0.4824, 0.5647,  ..., 0.5804, 0.2431, 0.2078],\n",
       "         [0.5647, 0.5059, 0.5569,  ..., 0.7216, 0.4627, 0.3608]],\n",
       "\n",
       "        [[0.2471, 0.1765, 0.1686,  ..., 0.4235, 0.4000, 0.4039],\n",
       "         [0.0784, 0.0000, 0.0000,  ..., 0.2157, 0.1961, 0.2235],\n",
       "         [0.0824, 0.0000, 0.0314,  ..., 0.1961, 0.1961, 0.1647],\n",
       "         ...,\n",
       "         [0.3765, 0.1333, 0.1020,  ..., 0.2745, 0.0275, 0.0784],\n",
       "         [0.3765, 0.1647, 0.1176,  ..., 0.3686, 0.1333, 0.1333],\n",
       "         [0.4549, 0.3686, 0.3412,  ..., 0.5490, 0.3294, 0.2824]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a4ed699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INPUT SHAPE:\n",
      "  Input image: torch.Size([1, 3, 32, 32])\n",
      "================================================================================\n",
      "\n",
      "ENCODER:\n",
      "--------------------------------------------------------------------------------\n",
      "  After encoder (flattened): torch.Size([1, 1024])\n",
      "  mu_min: torch.Size([1, 128])\n",
      "  mu_max: torch.Size([1, 128])\n",
      "  z (concatenated): torch.Size([1, 256])\n",
      "\n",
      "DECODER:\n",
      "--------------------------------------------------------------------------------\n",
      "  After decoder_input: torch.Size([1, 1024])\n",
      "  After reshape for decoder: torch.Size([1, 256, 2, 2])\n",
      "  After decoder: torch.Size([1, 32, 16, 16])\n",
      "  Final output: torch.Size([1, 3, 32, 32])\n",
      "\n",
      "================================================================================\n",
      "DETAILED LAYER-BY-LAYER SHAPES:\n",
      "================================================================================\n",
      "\n",
      "decoder_input_input:\n",
      "  [torch.Size([1, 256])]\n",
      "\n",
      "decoder_input_output:\n",
      "  torch.Size([1, 1024])\n",
      "\n",
      "decoder_layer_0_input:\n",
      "  [torch.Size([1, 256, 2, 2])]\n",
      "\n",
      "decoder_layer_0_output:\n",
      "  torch.Size([1, 128, 4, 4])\n",
      "\n",
      "decoder_layer_1_input:\n",
      "  [torch.Size([1, 128, 4, 4])]\n",
      "\n",
      "decoder_layer_1_output:\n",
      "  torch.Size([1, 64, 8, 8])\n",
      "\n",
      "decoder_layer_2_input:\n",
      "  [torch.Size([1, 64, 8, 8])]\n",
      "\n",
      "decoder_layer_2_output:\n",
      "  torch.Size([1, 32, 16, 16])\n",
      "\n",
      "encoder_layer_0_input:\n",
      "  [torch.Size([1, 3, 32, 32])]\n",
      "\n",
      "encoder_layer_0_output:\n",
      "  torch.Size([1, 32, 16, 16])\n",
      "\n",
      "encoder_layer_1_input:\n",
      "  [torch.Size([1, 32, 16, 16])]\n",
      "\n",
      "encoder_layer_1_output:\n",
      "  torch.Size([1, 64, 8, 8])\n",
      "\n",
      "encoder_layer_2_input:\n",
      "  [torch.Size([1, 64, 8, 8])]\n",
      "\n",
      "encoder_layer_2_output:\n",
      "  torch.Size([1, 128, 4, 4])\n",
      "\n",
      "encoder_layer_3_input:\n",
      "  [torch.Size([1, 128, 4, 4])]\n",
      "\n",
      "encoder_layer_3_output:\n",
      "  torch.Size([1, 256, 2, 2])\n",
      "\n",
      "fc_mu_max_input:\n",
      "  [torch.Size([1, 1024])]\n",
      "\n",
      "fc_mu_max_output:\n",
      "  torch.Size([1, 128])\n",
      "\n",
      "fc_mu_min_input:\n",
      "  [torch.Size([1, 1024])]\n",
      "\n",
      "fc_mu_min_output:\n",
      "  torch.Size([1, 128])\n",
      "\n",
      "final_layer_0_input:\n",
      "  [torch.Size([1, 32, 16, 16])]\n",
      "\n",
      "final_layer_0_output:\n",
      "  torch.Size([1, 32, 32, 32])\n",
      "\n",
      "final_layer_1_input:\n",
      "  [torch.Size([1, 32, 32, 32])]\n",
      "\n",
      "final_layer_1_output:\n",
      "  torch.Size([1, 32, 32, 32])\n",
      "\n",
      "final_layer_2_input:\n",
      "  [torch.Size([1, 32, 32, 32])]\n",
      "\n",
      "final_layer_2_output:\n",
      "  torch.Size([1, 32, 32, 32])\n",
      "\n",
      "final_layer_3_input:\n",
      "  [torch.Size([1, 32, 32, 32])]\n",
      "\n",
      "final_layer_3_output:\n",
      "  torch.Size([1, 3, 32, 32])\n",
      "\n",
      "final_layer_4_input:\n",
      "  [torch.Size([1, 3, 32, 32])]\n",
      "\n",
      "final_layer_4_output:\n",
      "  torch.Size([1, 3, 32, 32])\n",
      "\n",
      "================================================================================\n",
      "FULL FORWARD PASS RESULT:\n",
      "================================================================================\n",
      "  Reconstructed: torch.Size([1, 3, 32, 32])\n",
      "  Original: torch.Size([1, 3, 32, 32])\n",
      "  mu_min: torch.Size([1, 128])\n",
      "  mu_max: torch.Size([1, 128])\n",
      "\n",
      "================================================================================\n",
      "SPATIAL DIMENSION PROGRESSION:\n",
      "================================================================================\n",
      "  Input: 32x32\n",
      "  After encoder layers (stride=2 each):\n",
      "    Layer 0 (hidden_dim=256): 16x16\n",
      "    Layer 1 (hidden_dim=128): 8x8\n",
      "    Layer 2 (hidden_dim=64): 4x4\n",
      "    Layer 3 (hidden_dim=32): 2x2\n",
      "  Flattened size: 128\n",
      "  Latent dim: 128 (each of mu_min and mu_max)\n",
      "  Concatenated z: 256\n",
      "  After decoder layers (stride=2 each):\n",
      "    Initial decoder input: 2x2\n",
      "    After decoder layer 0: 4x4\n",
      "    After decoder layer 1: 8x8\n",
      "    After decoder layer 2: 16x16\n",
      "  Final output: 16x16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example configuration - you can modify these\n",
    "latent_dim = 128\n",
    "hidden_dims = [32, 64, 128, 256]  # Example hidden dimensions\n",
    "\n",
    "# Create model\n",
    "model = VanillaVAE(latent_dim=latent_dim, hidden_dims=hidden_dims)\n",
    "model.eval()\n",
    "\n",
    "# Get a CIFAR image\n",
    "sample_image, _ = ds[0]\n",
    "sample_image = sample_image.unsqueeze(0)  # Add batch dimension: (1, 3, 32, 32)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INPUT SHAPE:\")\n",
    "print(f\"  Input image: {sample_image.shape}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Hook function to capture intermediate shapes\n",
    "shapes_dict = {}\n",
    "\n",
    "def get_shape_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(input, tuple):\n",
    "            input_shapes = [inp.shape if hasattr(inp, 'shape') else str(inp) for inp in input]\n",
    "            shapes_dict[f\"{name}_input\"] = input_shapes\n",
    "        else:\n",
    "            shapes_dict[f\"{name}_input\"] = input.shape if hasattr(input, 'shape') else str(input)\n",
    "        \n",
    "        if isinstance(output, (list, tuple)):\n",
    "            output_shapes = [out.shape if hasattr(out, 'shape') else str(out) for out in output]\n",
    "            shapes_dict[f\"{name}_output\"] = output_shapes\n",
    "        else:\n",
    "            shapes_dict[f\"{name}_output\"] = output.shape if hasattr(output, 'shape') else str(output)\n",
    "    return hook\n",
    "\n",
    "# Register hooks on encoder layers\n",
    "for i, layer in enumerate(model.encoder):\n",
    "    layer.register_forward_hook(get_shape_hook(f\"encoder_layer_{i}\"))\n",
    "\n",
    "# Register hooks on fc layers\n",
    "model.fc_mu_min.register_forward_hook(get_shape_hook(\"fc_mu_min\"))\n",
    "model.fc_mu_max.register_forward_hook(get_shape_hook(\"fc_mu_max\"))\n",
    "model.decoder_input.register_forward_hook(get_shape_hook(\"decoder_input\"))\n",
    "\n",
    "# Register hooks on decoder layers\n",
    "for i, layer in enumerate(model.decoder):\n",
    "    layer.register_forward_hook(get_shape_hook(f\"decoder_layer_{i}\"))\n",
    "\n",
    "# Register hooks on final layer components\n",
    "for i, layer in enumerate(model.final_layer):\n",
    "    layer.register_forward_hook(get_shape_hook(f\"final_layer_{i}\"))\n",
    "\n",
    "# Forward pass\n",
    "print(\"\\nENCODER:\")\n",
    "print(\"-\" * 80)\n",
    "with torch.no_grad():\n",
    "    # Encode\n",
    "    encoded = model.encode(sample_image)\n",
    "    mu_min, mu_max = encoded\n",
    "    \n",
    "    print(f\"  After encoder (flattened): {model.encoder(sample_image).flatten(start_dim=1).shape}\")\n",
    "    print(f\"  mu_min: {mu_min.shape}\")\n",
    "    print(f\"  mu_max: {mu_max.shape}\")\n",
    "    print(f\"  z (concatenated): {torch.cat([mu_min, mu_max], dim=-1).shape}\")\n",
    "    \n",
    "    # Decode\n",
    "    z = torch.cat([mu_min, mu_max], dim=-1)\n",
    "    print(\"\\nDECODER:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    decoder_input = model.decoder_input(z)\n",
    "    print(f\"  After decoder_input: {decoder_input.shape}\")\n",
    "    \n",
    "    # Reshape for decoder\n",
    "    decoder_input_reshaped = decoder_input.view(-1, hidden_dims[0], 2, 2)\n",
    "    print(f\"  After reshape for decoder: {decoder_input_reshaped.shape}\")\n",
    "    \n",
    "    decoder_output = model.decoder(decoder_input_reshaped)\n",
    "    print(f\"  After decoder: {decoder_output.shape}\")\n",
    "    \n",
    "    final_output = model.final_layer(decoder_output)\n",
    "    print(f\"  Final output: {final_output.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DETAILED LAYER-BY-LAYER SHAPES:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Print all captured shapes\n",
    "for key in sorted(shapes_dict.keys()):\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(f\"  {shapes_dict[key]}\")\n",
    "\n",
    "# Also print the full forward pass result\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FULL FORWARD PASS RESULT:\")\n",
    "print(\"=\" * 80)\n",
    "with torch.no_grad():\n",
    "    result = model(sample_image)\n",
    "    reconstructed, original, mu_min, mu_max = result\n",
    "    print(f\"  Reconstructed: {reconstructed.shape}\")\n",
    "    print(f\"  Original: {original.shape}\")\n",
    "    print(f\"  mu_min: {mu_min.shape}\")\n",
    "    print(f\"  mu_max: {mu_max.shape}\")\n",
    "\n",
    "# Calculate spatial dimensions at each stage\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SPATIAL DIMENSION PROGRESSION:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Input: {sample_image.shape[2]}x{sample_image.shape[3]}\")\n",
    "\n",
    "# Calculate encoder spatial dimensions\n",
    "current_h, current_w = sample_image.shape[2], sample_image.shape[3]\n",
    "print(f\"  After encoder layers (stride=2 each):\")\n",
    "for i, h_dim in enumerate(hidden_dims):\n",
    "    current_h = current_h // 2\n",
    "    current_w = current_w // 2\n",
    "    print(f\"    Layer {i} (hidden_dim={h_dim}): {current_h}x{current_w}\")\n",
    "\n",
    "print(f\"  Flattened size: {hidden_dims[-1] * current_h * current_w}\")\n",
    "print(f\"  Latent dim: {latent_dim} (each of mu_min and mu_max)\")\n",
    "print(f\"  Concatenated z: {latent_dim * 2}\")\n",
    "\n",
    "# Calculate decoder spatial dimensions\n",
    "print(f\"  After decoder layers (stride=2 each):\")\n",
    "decoder_h, decoder_w = 2, 2  # Starting from decoder_input reshape\n",
    "print(f\"    Initial decoder input: {decoder_h}x{decoder_w}\")\n",
    "for i in range(len(hidden_dims) - 1):\n",
    "    decoder_h = decoder_h * 2\n",
    "    decoder_w = decoder_w * 2\n",
    "    print(f\"    After decoder layer {i}: {decoder_h}x{decoder_w}\")\n",
    "\n",
    "print(f\"  Final output: {decoder_h}x{decoder_w}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c9f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vh-crl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
